{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Required Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() #Create the spark session\n",
    "\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "RatingsSchema = StructType([\n",
    "    StructField(\"UserID\", IntegerType(), True),\n",
    "    StructField(\"ItemID\", IntegerType(), True),\n",
    "    StructField(\"Rating\", FloatType(), True)\n",
    "])\n",
    "\n",
    "TrustSchema = StructType([\n",
    "    StructField(\"TrustorUserID\", IntegerType(), True),\n",
    "    StructField(\"TrusteeUserID\", IntegerType(), True),\n",
    "    StructField(\"TrustValue\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "RatingsDF = spark.read.load(\"ratings.txt\", format='csv', sep=\" \", schema=RatingsSchema) #Read the contents of ratings.txt\n",
    "\n",
    "print(\"There are\", RatingsDF.count(),\"rows of data as standard\") #Display the number of rows in RatingsDF\n",
    "RatingsDF = RatingsDF.dropna() #Remove rows with empty data present\n",
    "print(\"There are\", RatingsDF.count(),\"rows of data after dropping empty data\") #Display the number of rows in RatingsDF so it can be compared to remove removing incomplete rows\n",
    "RatingsDF = RatingsDF.dropDuplicates(['UserID', 'ItemID']) #Remove rows that are duplicates of each other\n",
    "print(\"There are\", RatingsDF.count(),\"rows of data after dropping duplicates\") #Display the number of rows in RatingsDF so it can be comapred to see how much data cleaning has taken place\n",
    "\n",
    "#Repeat the entire process for the trust.txt file into a second dataframe\n",
    "TrustDF = spark.read.load(\"trust.txt\", format='csv', sep=\" \", schema=TrustSchema)\n",
    "\n",
    "print(\"There are\", TrustDF.count(), \"rows of data as standard\")\n",
    "TrustDF = TrustDF.dropna()\n",
    "print(\"There are\", TrustDF.count(), \"rows of data after dropping empty data\")\n",
    "TrustDF = TrustDF.dropDuplicates(['TrustorUserID', 'TrusteeUserID'])\n",
    "print(\"There are\", TrustDF.count(), \"rows of data after dropping duplicates\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "RatingsPandas = RatingsDF.toPandas()\n",
    "RatingsOnly = RatingsPandas['Rating']\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "bins=[-0.25, 0.25, 0.75, 1.25, 1.75, 2.25, 2.75, 3.25, 3.75, 4.25] #defining the bins so the histogram has very neat and clear bars\n",
    "fig, ax = plt.subplots(figsize = (9,9))#\n",
    "counts, edges, bars = plt.hist(RatingsOnly, bins=bins, edgecolor=\"black\") #plotting the histogram\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Times the Rating was given')\n",
    "plt.bar_label(bars) #adds the value of the bar as a label\n",
    "plt.show()\n",
    "#https://docs.kanaries.net/topics/PySpark/pyspark-dataframe-column-list\n",
    "#https://www.geeksforgeeks.org/how-to-plot-histogram-from-list-of-data-in-matplotlib/\n",
    "#https://stackoverflow.com/questions/39841733/matplotlib-histogram-how-to-display-the-count-over-the-bar\n",
    "#https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "\n",
    "\n",
    "dfR = RatingsDF.groupBy(\"ItemID\").agg(mean('Rating').alias(\"AverageRating\"), count('ItemID').alias(\"NumOfRatings\")) #Get the average rating and the number of times an item has been rated for each film\n",
    "dfR = dfR.filter(dfR.NumOfRatings > 10)\n",
    "\n",
    "dfTrustee = TrustDF.groupBy(\"TrusteeUserID\").agg(count('TrusteeUserID').alias(\"NumOfRatings\"))\n",
    "dfTrustee = dfTrustee.filter(dfTrustee.NumOfRatings > 5)\n",
    "\n",
    "dfRUsers = RatingsDF.groupBy(\"UserID\").agg(count('UserID').alias(\"RatingsGiven\"))\n",
    "\n",
    "print(\"Top 5 films\")\n",
    "dfR.orderBy(desc(\"AverageRating\")).show(5)\n",
    "print(\"Bottom 5 films\")\n",
    "dfR.orderBy(asc(\"AverageRating\")).show(5)\n",
    "\n",
    "print(\"The top 10 most rated films\")\n",
    "dfR.orderBy(desc(\"NumOfRatings\")).show(10)\n",
    "\n",
    "print(\"Top 15 users to be rated by other users\")\n",
    "dfTrustee.orderBy(desc(\"NumOfRatings\")).show(15)\n",
    "\n",
    "print(\"The top 10 users with the most ratings given\")\n",
    "dfRUsers.orderBy(desc(\"RatingsGiven\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/07 17:00:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/01/07 17:00:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 0.8452041954555893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------------+\n",
      "|UserID|recommendations                                                                             |\n",
      "+------+--------------------------------------------------------------------------------------------+\n",
      "|1     |[{1517, 4.9735093}, {1353, 4.61414}, {162, 4.500662}, {97, 4.500662}, {1111, 4.449911}]     |\n",
      "|3     |[{319, 4.098793}, {68, 4.030685}, {158, 3.9604828}, {145, 3.9604828}, {107, 3.9604828}]     |\n",
      "|5     |[{944, 3.3017063}, {487, 3.1185508}, {969, 3.0695858}, {208, 2.895045}, {1183, 2.8861465}]  |\n",
      "|6     |[{162, 5.239632}, {97, 5.239632}, {944, 5.1879244}, {1517, 5.0094934}, {1245, 4.954931}]    |\n",
      "|9     |[{312, 4.8559732}, {319, 4.824621}, {68, 4.824369}, {888, 4.479307}, {162, 4.376497}]       |\n",
      "|12    |[{1353, 5.128199}, {162, 4.938905}, {97, 4.938905}, {1535, 4.9277897}, {1517, 4.8858047}]   |\n",
      "|13    |[{1517, 4.9222584}, {162, 4.283011}, {97, 4.283011}, {464, 4.2118573}, {1111, 4.190349}]    |\n",
      "|15    |[{1517, 4.3712435}, {900, 4.296347}, {162, 4.284915}, {97, 4.284915}, {105, 4.1590066}]     |\n",
      "|16    |[{162, 4.5714235}, {97, 4.5714235}, {1245, 4.5354805}, {1508, 4.4775567}, {944, 4.459834}]  |\n",
      "|17    |[{312, 4.532683}, {530, 4.2639527}, {1517, 4.1119585}, {1360, 4.0160947}, {1687, 4.0113835}]|\n",
      "|19    |[{1517, 4.480393}, {162, 4.403508}, {97, 4.403508}, {1111, 4.1777315}, {105, 4.159074}]     |\n",
      "|20    |[{162, 3.1091855}, {97, 3.1091855}, {888, 3.0924788}, {316, 3.0699909}, {1535, 3.057152}]   |\n",
      "|22    |[{162, 5.6758046}, {97, 5.6758046}, {1517, 5.6228857}, {44, 5.2058697}, {105, 5.196723}]    |\n",
      "|26    |[{68, 4.772055}, {319, 4.633587}, {584, 4.5065637}, {189, 4.3288774}, {1068, 4.277136}]     |\n",
      "|27    |[{944, 3.82528}, {487, 3.8160176}, {464, 3.37872}, {1043, 3.3132174}, {909, 3.3120618}]     |\n",
      "|28    |[{312, 4.5144577}, {162, 4.198405}, {97, 4.198405}, {1517, 4.1797543}, {530, 3.9372706}]    |\n",
      "|31    |[{312, 4.414547}, {319, 4.388652}, {68, 4.3808103}, {1872, 3.8875968}, {162, 3.851051}]     |\n",
      "|34    |[{312, 5.5425453}, {888, 5.336481}, {126, 5.109633}, {894, 5.006426}, {867, 5.006426}]      |\n",
      "|35    |[{312, 4.1378264}, {162, 4.079135}, {97, 4.079135}, {319, 3.9000273}, {68, 3.8929403}]      |\n",
      "|37    |[{1517, 5.4011126}, {312, 5.1542835}, {530, 5.077773}, {1537, 5.027525}, {1001, 4.977577}]  |\n",
      "+------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(tries)):\n\u001b[1;32m     47\u001b[0m     als \u001b[38;5;241m=\u001b[39m ALS(\n\u001b[1;32m     48\u001b[0m         maxIter\u001b[38;5;241m=\u001b[39mtries[i],\n\u001b[1;32m     49\u001b[0m         regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m         nonnegative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     )\n\u001b[0;32m---> 57\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     train_predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(train_data)\n\u001b[1;32m     59\u001b[0m     test_predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#https://medium.com/@brunoborges_38708/recommender-system-using-als-in-pyspark-10329e1d1ee1\n",
    "#https://api-docs.databricks.com/python/pyspark/latest/api/pyspark.ml.recommendation.ALS.html\n",
    "\n",
    "#Splitting the data to be able to train the model\n",
    "train_data, test_data = RatingsDF.randomSplit([0.9, 0.1], seed=1234)\n",
    "\n",
    "#Using the ALS model\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    itemCol=\"ItemID\",\n",
    "    userCol=\"UserID\",\n",
    "    ratingCol=\"Rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "#Fitting the training data into the model\n",
    "model = als.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "#create evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"Rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "#using RMSE as the mathmatical evaluator of the entire model\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "\n",
    "#Make 5 recommendations for each user\n",
    "userRecommendations = model.recommendForAllUsers(5)\n",
    "userRecommendations.show(truncate=False)\n",
    "\n",
    "\n",
    "#testing the optimal number of iterations for ALS\n",
    "tries=[2,3,5,10,15,20,25]\n",
    "test_rmse=np.zeros((len(tries),1))\n",
    "train_rmse=np.zeros((len(tries),1))\n",
    "\n",
    "for i in range(0,len(tries)):\n",
    "    als = ALS(\n",
    "        maxIter=tries[i],\n",
    "        regParam=0.1,\n",
    "        itemCol=\"ItemID\",\n",
    "        userCol=\"UserID\",\n",
    "        ratingCol=\"Rating\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True\n",
    "    )\n",
    "\n",
    "    model = als.fit(train_data)\n",
    "    train_predictions = model.transform(train_data)\n",
    "    test_predictions = model.transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"Rating\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "\n",
    "    train_rmse[i]=evaluator.evaluate(train_predictions)\n",
    "    test_rmse[i]=evaluator.evaluate(test_predictions)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Root Mean Squared Error\")\n",
    "plt.plot(tries, train_rmse, 'b')\n",
    "plt.plot(tries, test_rmse, 'g')\n",
    "plt.axvline(x=10, color='r')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
