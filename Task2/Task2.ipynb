{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 16:33:23 WARN Utils: Your hostname, codespaces-8bb66a resolves to a loopback address: 127.0.0.1; using 10.0.11.17 instead (on interface eth0)\n",
      "24/12/03 16:33:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 16:33:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:========================================>                 (7 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'DEN': 46, 'CAN': 37, 'IAH': 37, 'ATL': 36, 'ORD': 33, 'KUL': 33, 'CGK': 27, 'JFK': 25, 'LHR': 25, 'CDG': 21, 'CLT': 21, 'PVG': 20, 'LAS': 17, 'BKK': 17, 'AMS': 15, 'FCO': 15, 'MUC': 14, 'MAD': 13, 'PEK': 13, 'HND': 13, 'DFW': 11, 'MIA': 11})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filename, columnNames):\n",
    "    df = pd.read_csv(filename,encoding='latin1',names=columnNames)\n",
    "    return df\n",
    "\n",
    "def get_instances(data):\n",
    "    instances = data.split()\n",
    "    return Counter(instances)\n",
    "\n",
    "def ReduceCounter(counter1, counter2):\n",
    "    counter1.update(counter2)\n",
    "    return counter1\n",
    "\n",
    "conf = SparkConf().setAppName('MapReduce').setMaster('local')\n",
    "sparkContext = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "columns = ['passengerID', 'flightID', 'originAirport', 'destinationAirport', 'departureTime', 'flightTime']\n",
    "\n",
    "rdd = load_dataset(\"AComp_Passenger_data_no_error.csv\", columns)\n",
    "\n",
    "Origins = rdd['originAirport'].tolist()\n",
    "distributed_data_origins = sparkContext.parallelize(Origins, 10)\n",
    "\n",
    "dist_data_flight_origins = distributed_data_origins.map(get_instances)\n",
    "dist_data_flight_origins_count = dist_data_flight_origins.reduce(ReduceCounter)\n",
    "print(dist_data_flight_origins_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Flight ID, Count of Passengers, Departure code, Departure Time, Arrival Code, Arrival Time\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def GetFlightInfo(acc, record):\n",
    "    flightID = record['flightID']\n",
    "\n",
    "    if flightID not in acc:\n",
    "        acc[flightID] = record\n",
    "    return acc\n",
    "\n",
    "airportTimezones = {\n",
    "    \"ATL\": \"America/New_York\",\n",
    "    \"PEK\": \"Asia/Shanghai\",\n",
    "    \"LHR\": \"Europe/London\",\n",
    "    \"ORD\": \"America/Chicago\",\n",
    "    \"HND\": \"Asia/Tokyo\",\n",
    "    \"LAX\": \"America/Los_Angeles\",\n",
    "    \"CDG\": \"Europe/Paris\",\n",
    "    \"DFW\": \"America/Chicago\",\n",
    "    \"FRA\": \"Europe/Berlin\",\n",
    "    \"HKG\": \"Asia/Hong_Kong\",\n",
    "    \"DEN\": \"America/Denver\",\n",
    "    \"DXB\": \"Asia/Dubai\",\n",
    "    \"CGK\": \"Asia/Jakarta\",\n",
    "    \"AMS\": \"Europe/Amsterdam\",\n",
    "    \"MAD\": \"Europe/Madrid\",\n",
    "    \"BKK\": \"Asia/Bangkok\",\n",
    "    \"JFK\": \"America/New_York\",\n",
    "    \"SIN\": \"Asia/Singapore\",\n",
    "    \"CAN\": \"Asia/Shanghai\",\n",
    "    \"LAS\": \"America/Los_Angeles\",\n",
    "    \"PVG\": \"Asia/Shanghai\",\n",
    "    \"SFO\": \"America/Los_Angeles\",\n",
    "    \"PHX\": \"America/Phoenix\",\n",
    "    \"IAH\": \"America/Chicago\",\n",
    "    \"CLT\": \"America/New_York\",\n",
    "    \"MIA\": \"America/New_York\",\n",
    "    \"MUC\": \"Europe/Berlin\",\n",
    "    \"KUL\": \"Asia/Kuala_Lumpur\",\n",
    "    \"FCO\": \"Europe/Rome\",\n",
    "    \"IST\": \"Europe/Istanbul\"\n",
    "}\n",
    "\n",
    "def CalculateTimes(acc, record):\n",
    "    flightID, originAirport, departureTime, destinationAirport, flightTime = record #get parts of the record that are needed\n",
    "\n",
    "    depRealTime = datetime.fromtimestamp(departureTime, timezone(airportTimezones[originAirport])) #apply timezone to destination\n",
    "    flightDelta = timedelta(minutes=flightTime) #work out the minutes from the flight time\n",
    "\n",
    "    arrivalTime = depRealTime + flightDelta #calculate arrival time\n",
    "    arrivalTime = arrivalTime.astimezone(timezone(airportTimezones[destinationAirport])) #apply destination timezone\n",
    "    arrivalTimeFormat = arrivalTime.strftime('%H:%M') #format time\n",
    "    depTimeFormat = depRealTime.strftime('%H:%M')\n",
    "\n",
    "    acc.append({\n",
    "        \"flightID\": flightID,\n",
    "        \"departureTime\": depTimeFormat,\n",
    "        \"arrivalTime\": arrivalTimeFormat\n",
    "    })\n",
    "\n",
    "    return acc\n",
    "\n",
    "Flights = rdd['flightID'].tolist()\n",
    "distributed_data_flights = sparkContext.parallelize(Flights, 10)\n",
    "\n",
    "dist_data_flight_id = distributed_data_flights.map(get_instances)\n",
    "dist_data_flight_id_count = dist_data_flight_id.reduce(ReduceCounter)\n",
    "\n",
    "dist_data_flight_id_list = list(dist_data_flight_id_count.keys())\n",
    "\n",
    "#Getting data about each flight\n",
    "FirstFlightInfo = reduce(GetFlightInfo, [record for _, record in rdd.iterrows()], {})\n",
    "flightIDs = [record['flightID'] for record in FirstFlightInfo.values()]\n",
    "originAirports = [record['originAirport'] for record in FirstFlightInfo.values()]\n",
    "departureTimes = [record['departureTime'] for record in FirstFlightInfo.values()]\n",
    "destinationAirports = [record['destinationAirport'] for record in FirstFlightInfo.values()]\n",
    "flightTimes = [record['flightTime'] for record in FirstFlightInfo.values()]\n",
    "FlightInfo = pd.DataFrame({\n",
    "    'flightID': flightIDs,\n",
    "    'originAirport': originAirports,\n",
    "    'departureTime': departureTimes,\n",
    "    'destinationAirport': destinationAirports,\n",
    "    'flightTime': flightTimes\n",
    "})\n",
    "\n",
    "#get the flightID and number of Passengers \n",
    "FlightData = pd.DataFrame({\n",
    "    'flightID': list(dist_data_flight_id_count.keys()),\n",
    "    'passengerCount': list(dist_data_flight_id_count.values())\n",
    "})\n",
    "\n",
    "df = pd.merge(FlightInfo, FlightData, on='flightID', how='inner')\n",
    "\n",
    "FlightInfoTuples = FlightInfo[['flightID', 'originAirport', 'departureTime', 'destinationAirport', 'flightTime']].itertuples(index=False) #Convert to tuples so can use reduce\n",
    "\n",
    "CalcTime = reduce(CalculateTimes, FlightInfoTuples, []) #use a reduce with tuples input and the function CalculateTimes\n",
    "CalcTimeFI = [flight['flightID'] for flight in CalcTime]\n",
    "CalcTimeDT = [flight['departureTime'] for flight in CalcTime]\n",
    "CalcTimeAT = [flight['arrivalTime'] for flight in CalcTime]\n",
    "\n",
    "CalcArrTime = pd.DataFrame({\n",
    "    'flightID': CalcTimeFI,\n",
    "    'departureTime': CalcTimeDT,\n",
    "    'arrivalTime': CalcTimeAT\n",
    "})\n",
    "\n",
    "df = pd.merge(df, CalcArrTime, on='flightID', how='inner', suffixes=('_orig', '_new'))\n",
    "df['departureTime_orig'] = df['departureTime_new']\n",
    "df = df.drop(columns=['departureTime_new'])\n",
    "df = df.rename(columns={\"departureTime_orig\": \"departureTime\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
